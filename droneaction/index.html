<!-- The source code is from: https://gkioxari.github.io/ -->


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Drone-Action</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="../css/style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>

</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="623" align="center" valign="middle"><h3> Drone-Action: An Outdoor Recorded Drone Video Dataset for Action Recognition</h3></td></tr>
    <tr><td width="300" align="center" valign="middle"> <a href="https://asankagp.github.io/">Asanka G Perera</a>, Yee Wei Law, Javaan Chahl</td></tr>
  </table>
  </br>
  <p><img src="action_classes.png" width="600" align="middle" /></p>
</div>

</br>

<div class="container">
  <h2>Abstract</h2>
    <p>Aerial human action recognition is an emerging topic in drone applications. Commercial drone platforms capable of detecting
    basic human actions such as hand gestures have been developed. However, a limited number of aerial video datasets are available
    to support increased research in to aerial human action analysis. Most of the datasets are confined to indoor scenes or object
    tracking and many outdoor datasets do not have sufficient human body details to apply state-of-the-art machine learning techniques.
    To fill this gap and enable research in wider application areas, we present an action recognition dataset recorded in an
    outdoor setting. A free flying drone was used to record 13 dynamic human actions. The dataset contains 240 high-definition
    video clips consisting of 66919 frames. All of the videos were recorded from low-altitude and slow speed to capture the maximum
    human pose details with relatively high resolution. This dataset should be useful to many research areas including action
    recognition, surveillance, situational awareness and gait analysis. To test the dataset, we evaluated the dataset with a
    pose-based convolutional neural network (P-CNN) and high-level pose feature (HLPF) descriptors. The overall baseline action
    recognition accuracy calculated using P-CNN was 75.92%.</p> 

</div>

</br>

<div class="container" text-align="left">
  <h2>Download</h2>
    <table border="0" align="left">
      <tr><td width="1000" align="left">The dataset will be available soon.</br></br> 
    </table> 
</div>

</br>

<div class="containersmall">
  <p>Contact: <a href="mailto:asanka.perera@mymail.unisa.edu.au">Asanka Perera</a></p>
</div>
 


</body>
</html>
